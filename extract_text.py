
from bs4 import BeautifulSoup
import json

with open("temp_reference_report.html", "r") as f:
    html = f.read()

soup = BeautifulSoup(html, "html.parser")

section_map = {}

# Find "Pre-Match Interview" sections
# The structure might be: <h3>Home Team Interview</h3> ... <p>...</p>
# Or look for specific keywords if headers aren't clear.
# Based on common structures, let's dump all h3 and following p text to identify fields.

def get_section_text(header_text):
    header = soup.find(lambda tag: tag.name in ["h3", "h4", "h5"] and header_text in tag.get_text())
    if header:
        content = []
        curr = header.find_next_sibling()
        while curr and curr.name != "h3" and curr.name != "h4" and curr.name != "h5":
            if curr.name == "p":
                 content.append(curr.get_text(strip=True))
            elif curr.name == "ul":
                 for li in curr.find_all("li"):
                     content.append("- " + li.get_text(strip=True))
            curr = curr.find_next_sibling()
        return "\n\n".join(content)
    return ""

extracted = {}

# Extract based on specific headers found
home_interview_headers = [
    'æ–°æˆ¦åŠ›ã‚¢ãƒ³ãƒˆãƒ¯ãƒ¼ãƒŒãƒ»ã‚»ãƒ¡ãƒ‹ãƒ§ã«ã¤ã„ã¦', 
    'æ”»æ’ƒé™£ã®ç¾çŠ¶ã¨ã‚»ãƒ¡ãƒ‹ãƒ§ç²å¾—ã®èƒŒæ™¯', 
    'è² å‚·è€…ã®çŠ¶æ³', 
    'å¯¾æˆ¦ç›¸æ‰‹ã‚¨ã‚¯ã‚»ã‚¿ãƒ¼ãƒ»ã‚·ãƒ†ã‚£ã«ã¤ã„ã¦'
]
away_interview_headers = [
    'FAã‚«ãƒƒãƒ—3å›žæˆ¦ã€ãƒžãƒ³ãƒã‚§ã‚¹ã‚¿ãƒ¼ãƒ»ã‚·ãƒ†ã‚£æˆ¦ã«å‘ã‘ã¦', 
    '7,800äººã®ã‚µãƒãƒ¼ã‚¿ãƒ¼ã¨ã¨ã‚‚ã«', 
    'ãƒžãƒ³ãƒã‚§ã‚¹ã‚¿ãƒ¼ãƒ»ãƒ¦ãƒŠã‚¤ãƒ†ãƒƒãƒ‰ã®æ–½è¨­ã‚’åˆ©ç”¨ã—ã¦æº–å‚™', 
    'ãƒãƒ¼ãƒ ã®æœ€æ–°æƒ…å ±', 
    'ç›¸æ‰‹ãƒãƒ¼ãƒ ã®çŠ¶æ³'
]

home_interview_text = []
for h in home_interview_headers:
    text = get_section_text(h)
    if text:
        home_interview_text.append(f"### {h}\n{text}")

away_interview_text = []
for h in away_interview_headers:
    text = get_section_text(h)
    if text:
        away_interview_text.append(f"### {h}\n{text}")

extracted["home_interview"] = "\n\n".join(home_interview_text)
extracted["away_interview"] = "\n\n".join(away_interview_text)

# Transfer news likely under 'ðŸ†• ç²å¾—ãƒ»åŠ å…¥ (In)' which appears twice.
# We need to distinguish home and away based on position in file or preceding headers.
# Simplification: Find 'Manchester City' header then look for transfer sections until 'Exeter City'
# But headers list shows flat structure.
# Let's try to grab text under 'ðŸ†• ç²å¾—ãƒ»åŠ å…¥ (In)', 'ðŸ’¬ å™‚ãƒ»å‹•å‘ (Rumors)'
# The list shows: ... 'ðŸ†• ç²å¾—ãƒ»åŠ å…¥ (In)', 'ðŸ’¬ å™‚ãƒ»å‹•å‘ (Rumors)', 'ãã®ä»–', 'ðŸ†• ç²å¾—ãƒ»åŠ å…¥ (In)', 'ðŸ‘‹ æ”¾å‡ºãƒ»é€€å›£ (Out)', 'ðŸ’¬ å™‚ãƒ»å‹•å‘ (Rumors)' ...
# First group is likely Home (City), Second is Away (Exeter) or vice versa.
# Report order is usually Home then Away.

transfer_headers = ['ðŸ†• ç²å¾—ãƒ»åŠ å…¥ (In)', 'ðŸ‘‹ æ”¾å‡ºãƒ»é€€å›£ (Out)', 'ðŸ’¬ å™‚ãƒ»å‹•å‘ (Rumors)', 'ãã®ä»–']
# Find all occurrences
all_transfers = []
for h in transfer_headers:
    # This naive get_section_text only finds first. Use find_all loop.
    pass

# Better approach for transfer: Iterate through soup to split by team sections if possible.
# Given constraints, let's just dump specific known sections.

def get_all_sections(header_text):
    sections = []
    for header in soup.find_all(lambda tag: tag.name in ["h3", "h4", "h5", "h6"] and header_text in tag.get_text()):
        content = []
        curr = header.find_next_sibling()
        while curr and curr.name not in ["h2", "h3", "h4", "h5", "h6"]:
             if curr.name == "p":
                 content.append(curr.get_text(strip=True))
             elif curr.name == "ul":
                 for li in curr.find_all("li"):
                     content.append("- " + li.get_text(strip=True))
             curr = curr.find_next_sibling()
        sections.append("\n".join(content))
    return sections

in_news = get_all_sections('ðŸ†• ç²å¾—ãƒ»åŠ å…¥ (In)')
rumor_news = get_all_sections('ðŸ’¬ å™‚ãƒ»å‹•å‘ (Rumors)')
out_news = get_all_sections('ðŸ‘‹ æ”¾å‡ºãƒ»é€€å›£ (Out)')

# Assemble broadly
extracted["home_transfer_raw"] = "### åŠ å…¥\n" + (in_news[0] if in_news else "") + "\n\n### å™‚\n" + (rumor_news[0] if rumor_news else "")
extracted["away_transfer_raw"] = "### åŠ å…¥\n" + (in_news[1] if len(in_news)>1 else "") + "\n\n### æ”¾å‡º\n" + (out_news[0] if out_news else "") + "\n\n### å™‚\n" + (rumor_news[1] if len(rumor_news)>1 else "")

with open("extraction_output.json", "w", encoding="utf-8") as f:
    json.dump(extracted, f, indent=2, ensure_ascii=False)
